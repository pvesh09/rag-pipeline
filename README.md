# RAG Pipeline with Ollama

## Overview
This project implements a Retrieval-Augmented Generation (RAG) pipeline combining document ingestion, vector search, and local LLM generation using Ollama. The system allows uploading `.txt` and `.pdf` files, chunking them, embedding with SentenceTransformers, performing FAISS vector search, and generating context-aware answers with the Ollama LLM model.

## Features
- Supports `.txt` and `.pdf` document upload and processing
- Text chunking with overlap for better context retrieval
- Embeddings generated using `all-MiniLM-L6-v2` model from SentenceTransformers
- Efficient vector similarity search using FAISS
- Ollama LLM integration for answer generation strictly based on retrieved context
- Interactive Streamlit frontend for easy uploads and querying

## Installation

### Prerequisites
- Python 3.8 or higher
- Ollama app installed and running locally with pulled model (e.g., `llama3`)

### Clone Repository
git clone https://github.com/your-username/rag-pipeline-ollama.git
cd rag-pipeline-ollama

### Setup Virtual Environment and Install Dependencies
python -m venv .venv

Activate the environment
Windows:
.venv\Scripts\activate

Linux/macOS:
source .venv/bin/activate

pip install -r requirements.txt

*Note: Create a `requirements.txt` containing:*
streamlit
faiss-cpu
sentence-transformers
pdfplumber
ollama

---

## Usage

### Run Streamlit App
streamlit run app.py

### In the app
- Upload `.txt` or `.pdf` documents.
- Enter your question in the input box.
- View retrieved document chunks.
- View context-aware answers generated by the Ollama LLM.

---

## Project Structure

rag-demo/
│
├── docs/ # Place your documents (.txt/.pdf) here
├── src/
│ ├── ingestion.py # Document loading and chunking logic
│ ├── embeddings.py # Embedding model loading and generating vectors
│ ├── vector_store.py # Build and query FAISS vector index
│ ├── rag_pipeline.py # Ollama LLM answer generation code
│ └── main.py # (Optional) Pipeline runner for CLI
├── app.py # Streamlit frontend app integrating all modules
├── requirements.txt # Python dependencies
└── .gitignore # Files/folders ignored by Git


---

## Additional Notes
- Ensure Ollama app is running locally before querying in the app.
- For best results, upload meaningful documents covering your domain.
- The current chunk size and overlap parameters can be tweaked in `ingestion.py` for improved retrieval quality.
- The Streamlit app supports multiple uploads and merges documents for searching.

---

## Future Work / Bonus Features
- Deploy app to Hugging Face Spaces or other cloud hosting platforms.
- Introduce user authentication for secure access.
- Save query and response history to build a conversational chatbot experience.
- Add more advanced prompting and LLM model selection options.

---

## License
Licensed under MIT License (or specify your preferred license).

---

## Acknowledgements
- [Ollama](https://ollama.com) for local LLM capabilities
- [SentenceTransformers](https://www.sbert.net) for embeddings
- [FAISS](https://faiss.ai) for vector similarity search
- [Streamlit](https://streamlit.io) for the interactive UI framework
- PDF text extraction powered by `pdfplumber`

---

Feel free to contact at [your-email@example.com] for questions or collaboration.

